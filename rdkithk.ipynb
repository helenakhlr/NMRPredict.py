{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import MolToSmiles\n",
    "from rdkit.Chem import PandasTools\n",
    "import pandas as pd\n",
    "import os \n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orts: iNEXTG2-Plate-01 - iNEXTG2-Plate-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate molecule images from SMILES + save them with corresponding txt.row of the SMILES code\n",
    "def generate_molecule_images(dataframe, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'molecule_info.txt'), 'w') as out_file:\n",
    "        for index, row in dataframe.iterrows():\n",
    "            smiles = row['SMILES']  \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                img = Draw.MolToImage(mol)\n",
    "                img.save(os.path.join(output_folder, f'molecule_{index}.png'))  \n",
    "                out_file.write(f\"{index}, {smiles}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all necessary imports are done earlier in the notebook\n",
    "# Data extraction (e.g. Solvent and .mol) from SDF file, storing in CSV file\n",
    "# Converting .mol from SDF to SMILES in CSV  \n",
    "\n",
    "for i in range(1, 9):\n",
    "    plate_number = f\"0{i}\"\n",
    "    sdf_file_path = f\"datasets/raw/orts/iNEXT_Lib_1D/iNEXTG2-Plate-{plate_number}.SDF\"\n",
    "    csv_file_path = f\"datasets/processed/orts/iNEXT_Lib_1D/iNEXTG2-Plate-{plate_number}.csv\"\n",
    "    output_folder_path = f\"datasets/processed/orts/images/images_iNEXTG2-Plate-{plate_number}\"\n",
    "\n",
    "    # Load the SDF file into a DataFrame\n",
    "    sdf_supplier = Chem.SDMolSupplier(sdf_file_path)\n",
    "    mols = [mol for mol in sdf_supplier if mol is not None]\n",
    "    sdf_df = PandasTools.LoadSDF(sdf_file_path)\n",
    "\n",
    "    # Process DataFrame, Extraction of Information, .mol from SDF into SMILES for CSV\n",
    "    sdf_df[\"Solvent\"] = \"DMSO\"\n",
    "    sdf_df[\"SMILES\"] = [Chem.MolToSmiles(mol) for mol in mols]\n",
    "    sdf_df[\"SDF present\"] = \"iNEXT_Lib_1D\"\n",
    "    sdf_df[\"SDF file path\"] = sdf_file_path\n",
    "\n",
    "    sdf_df.to_csv(csv_file_path, index=False, mode=\"w\")\n",
    "\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Generate molecule images for SMILES, save them to CSV\n",
    "    generate_molecule_images(df, output_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nmrshiftdb2: nmredata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Chem.SanitizeFlags.SANITIZE_ALL ^ Chem.SanitizeFlags.SANITIZE_PROPERTIES\n",
    "sdf_supplier = Chem.SDMolSupplier('datasets/raw/nmrshiftdb2/nmredata/nmrshiftdb2.nmredata.sd', sanitize=True, removeHs=False)\n",
    "\n",
    "mols = []\n",
    "solvent_info = []\n",
    "\n",
    "for mol in sdf_supplier:\n",
    "    if mol is not None:\n",
    "        smiles = Chem.MolToSmiles(mol)\n",
    "        mols.append(smiles)\n",
    "\n",
    "        if 'NMREDATA_SOLVENT' in mol.GetPropsAsDict():\n",
    "            solvent = mol.GetPropsAsDict()['NMREDATA_SOLVENT']\n",
    "        else:\n",
    "            solvent = None\n",
    "        solvent_info.append(solvent)\n",
    "\n",
    "new_data = {\n",
    "    \"SMILES\": mols,\n",
    "    \"SDF present\": \"nmredata\",\n",
    "    \"SDF file path\": \"/data/shared/projects/nmr2structure/datasets/raw/nmrshfitdb2/nmredata/nmrshiftdb2.nmredata.sd\"\n",
    "}\n",
    "new_data[\"NMREDATA_SOLVENT\"] = solvent_info\n",
    "new_df = pd.DataFrame(new_data)\n",
    "new_df.rename(columns={\"NMREDATA_SOLVENT\": \"Solvent\"}, inplace=True)\n",
    "\n",
    "new_df.to_csv(\"datasets/processed/nmrshiftdb2/nmredata/nmrshiftdb2.nmredata.csv\", index=False, mode=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'datasets/processed/nmrshiftdb2/nmredata/nmrshiftdb2.nmredata.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "def generate_molecule_images(dataframe, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'molecule_info.txt'), 'w') as out_file:\n",
    "        for index, row in dataframe.iterrows():\n",
    "            smiles = row['SMILES']  \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                img = Draw.MolToImage(mol)\n",
    "                img.save(os.path.join(output_folder, f'molecule_{index}.png'))  \n",
    "                out_file.write(f\"{index}, {smiles}\\n\")\n",
    "\n",
    "output_folder_path = 'datasets/processed/nmrshiftdb2/nmredata/images.nmredata/'  \n",
    "generate_molecule_images(df, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'datasets/processed/nmrshiftdb2/nmredata/nmrshiftdb2.nmredata.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "image_folder = '/data/shared/projects/nmr2structure/datasets/processed/nmrshiftdb2/nmredata/images.nmredata/' \n",
    "df['Image folder location'] = df.index.map(lambda x: os.path.join(image_folder, f'molecule_{x}.png'))\n",
    "\n",
    "updated_csv_file_path = 'datasets/processed/nmrshiftdb2/nmredata/nmrshiftdb2.nmredata.csv'  \n",
    "df.to_csv(updated_csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nmrshiftdb2: withsignals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_supplier = Chem.SDMolSupplier(\"datasets/raw/nmrshiftdb2/withsignals/nmrshiftdb2withsignals.sd\")\n",
    "mols = [mol for mol in sdf_supplier if mol is not None]\n",
    "sdf_df = PandasTools.LoadSDF(\"datasets/raw/nmrshiftdb2/withsignals/nmrshiftdb2withsignals.sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_df[\"SMILES\"] = [Chem.MolToSmiles(mol) for mol in mols]\n",
    "sdf_df[\"SDF present\"] = \"withsignals\"\n",
    "sdf_df[\"SDF file path\"] = \"/data/shared/projects/nmr2structure/datasets/raw/nmrshiftdb2/withsignals/nmrshiftdb2withsignals.sd\"\n",
    "sdf_df.to_csv(\"datasets/processed/nmrshiftdb2/withsignals/nmrshiftdb2withsignals.csv\", index=False, mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'datasets/processed/nmrshiftdb2/withsignals/nmrshiftdb2withsignals.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "def generate_molecule_images(dataframe, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'molecule_info.txt'), 'w') as out_file:\n",
    "        for index, row in dataframe.iterrows():\n",
    "            smiles = row['SMILES']  \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                img = Draw.MolToImage(mol)\n",
    "                img.save(os.path.join(output_folder, f'molecule_{index}.png'))  \n",
    "                out_file.write(f\"{index}, {smiles}\\n\")\n",
    "\n",
    "output_folder_path = 'datasets/processed/nmrshiftdb2/withsignals/images.withsignals/'  \n",
    "\n",
    "generate_molecule_images(df, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'datasets/processed/nmrshiftdb2/withsignals/nmrshiftdb2withsignals.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "image_folder = '/data/shared/projects/nmr2structure/datasets/processed/nmrshiftdb2/withsignals/images.withsignals/' \n",
    "df['Image folder location'] = df.index.map(lambda x: os.path.join(image_folder, f'molecule_{x}.png'))\n",
    "\n",
    "updated_csv_file_path = 'datasets/processed/nmrshiftdb2/withsignals/nmrshiftdb2withsignals.csv'  \n",
    "df.to_csv(updated_csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nmrshiftdb2: identical pairs check in nmredata & withsignals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate hash of image file\n",
    "def calculate_hash(file_path):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        while True:\n",
    "            data = file.read(4096)\n",
    "            if not data:\n",
    "                break\n",
    "            hasher.update(data)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "folder1 = \"datasets/processed/nmrshiftdb2/nmredata/images.nmredata\"\n",
    "folder2 = \"datasets/processed/nmrshiftdb2/withsignals/images.withsignals\"\n",
    "\n",
    "# Initializing dictionaries to store image hashes\n",
    "hashes_folder1 = {}\n",
    "hashes_folder2 = {}\n",
    "\n",
    "# Populating dictionaries w image hashes from folder1\n",
    "for root, _, files in os.walk(folder1):\n",
    "    for file in files:\n",
    "        if file.endswith((\".jpg\", \".png\", \".gif\", \".bmp\")):\n",
    "            file_path = os.path.join(root, file)\n",
    "            image_hash = calculate_hash(file_path)\n",
    "            hashes_folder1[image_hash] = file_path\n",
    "\n",
    "# from folder2\n",
    "for root, _, files in os.walk(folder2):\n",
    "    for file in files:\n",
    "        if file.endswith((\".jpg\", \".png\", \".gif\", \".bmp\")):\n",
    "            file_path = os.path.join(root, file)\n",
    "            image_hash = calculate_hash(file_path)\n",
    "            hashes_folder2[image_hash] = file_path\n",
    "\n",
    "# Find identical image hashes\n",
    "identical_hashes = set(hashes_folder1.keys()) & set(hashes_folder2.keys())\n",
    "\n",
    "csv_filename = \"datasets/processed/nmrshiftdb2/identical_pairs_images.csv\"\n",
    "with open(csv_filename, mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"nmredata\", \"withsignals\"])\n",
    "\n",
    "    for image_hash in identical_hashes:\n",
    "        folder1_image = hashes_folder1.get(image_hash)\n",
    "        folder2_image = hashes_folder2.get(image_hash)\n",
    "        if folder1_image and folder2_image:\n",
    "            image_name1 = os.path.splitext(os.path.basename(folder1_image))[0]\n",
    "            image_name2 = os.path.splitext(os.path.basename(folder2_image))[0]\n",
    "            csv_writer.writerow([image_name1, image_name2])\n",
    "\n",
    "print(f\"Number of identical pairs found: {len(identical_hashes)}\")\n",
    "print(f\"CSV file '{csv_filename}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/processed/nmrshiftdb2/identical_pairs_images.csv\")\n",
    "\n",
    "def extract_and_add_2(image_name):\n",
    "    try:\n",
    "        number = int(image_name.split('_')[1])\n",
    "        new_number = number + 2\n",
    "        return new_number\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df[\"Original row nmredata\"] = df[\"nmredata\"].apply(extract_and_add_2)\n",
    "df[\"Original row withsignals\"] = df[\"withsignals\"].apply(extract_and_add_2)\n",
    "\n",
    "df.to_csv(\"datasets/processed/nmrshiftdb2/identical_pairs_images.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file1 = \"datasets/processed/nmrshiftdb2//nmredata/nmrshiftdb2.nmredata.csv\"\n",
    "csv_file2 = \"datasets/processed/nmrshiftdb2/withsignals/nmrshiftdb2withsignals.csv\"\n",
    "\n",
    "df1 = pd.read_csv(csv_file1)\n",
    "df2 = pd.read_csv(csv_file2)\n",
    "\n",
    "smiles1 = df1[\"SMILES\"]\n",
    "smiles2 = df2[\"SMILES\"]\n",
    "\n",
    "unique_smiles2 = set(smiles2)\n",
    "\n",
    "identical_pairs_count = 0\n",
    "identical_pairs = []\n",
    "\n",
    "for idx, smile1 in enumerate(smiles1):\n",
    "    mol1 = Chem.MolFromSmiles(smile1)\n",
    "    if mol1:\n",
    "        identical_in_smiles2 = [smile2 for smile2 in unique_smiles2 if smile2 == smile1]\n",
    "        if identical_in_smiles2:\n",
    "            identical_pairs_count += 1\n",
    "            identical_pairs.append((idx, [df2.index[df2[\"SMILES\"] == smile2].tolist() for smile2 in identical_in_smiles2]))\n",
    "\n",
    "print(\"Number of identical pairs:\", identical_pairs_count)\n",
    "\n",
    "identical_pairs_df = pd.DataFrame(identical_pairs, columns=[\"nmredata\", \"withsignals\"])\n",
    "\n",
    "identical_pairs_df.to_csv(\"datasets/processed/nmrshiftdb2/identical_pairs.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = pd.read_csv(\"datasets/processed/nmrshiftdb2/identical_pairs_images.csv\")\n",
    "pairs_df = pd.read_csv(\"datasets/processed/nmrshiftdb2/identical_pairs.csv\")\n",
    "\n",
    "def extract_number_withsignals(withsignals):\n",
    "    match = re.search(r'\\[\\[(\\d+)]]', withsignals)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "pairs_df[\"withsignals_number\"] = pairs_df[\"withsignals\"].apply(extract_number_withsignals)\n",
    "\n",
    "unique_number_pairings = set(zip(pairs_df[\"nmredata\"], pairs_df[\"withsignals_number\"]))\n",
    "\n",
    "identical_pairs = []\n",
    "\n",
    "for _, row in images_df.iterrows():\n",
    "    nmredata = row[\"Original row nmredata\"]\n",
    "    withsignals = row[\"Original row withsignals\"]\n",
    "    if (nmredata, withsignals) in unique_number_pairings:\n",
    "        identical_pairs.append({\"nmredata\": nmredata, \"withsignals\": withsignals})\n",
    "\n",
    "identical_pairs_df = pd.DataFrame(identical_pairs)\n",
    "\n",
    "identical_pairs_df.to_csv(\"datasets/processed/nmrshiftdb2/identical_pairs_combined.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urban files: AG VB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gekaufte Substanzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"datasets/processed/urban/AG Battisti Verena P/Gekaufte Substanzen/Gekaufte Substanzen_raw.txt\"\n",
    "output_file = \"datasets/processed/urban/AG Battisti Verena P/Gekaufte Substanzen/Gekaufte Substanzen_extracted.csv\"\n",
    "\n",
    "ids = []\n",
    "compound_names = [] \n",
    "pattern = r\"^VB-\\d{4}\"\n",
    "\n",
    "with open(input_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if re.search(pattern, line):\n",
    "        ids.append(line.strip())\n",
    "        \n",
    "        if i >= 2:\n",
    "            compound_name = lines[i - 2].strip()\n",
    "        else:\n",
    "            compound_name = \"\"\n",
    "        compound_names.append(compound_name)\n",
    "\n",
    "data = [{\"ID\": id_value, \"compound name\": comp_name} for id_value, comp_name in zip(ids, compound_names)]\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = [\"ID\", \"compound name\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Extracted data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"datasets/processed/urban/AG Battisti Verena P/VB- Battisti Verena/VB_raw.txt\"\n",
    "output_file = \"datasets/processed/urban/AG Battisti Verena P/VB- Battisti Verena/VB_extracted.csv\"\n",
    "\n",
    "ids = []\n",
    "compound_names = []\n",
    "\n",
    "pattern = r\"^VB-\\d{3}\"\n",
    "\n",
    "with open(input_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if re.search(pattern, line):\n",
    "        ids.append(line.strip())\n",
    "        \n",
    "        if i >= 2:\n",
    "            compound_name = lines[i - 2].strip()\n",
    "        else:\n",
    "            compound_name = \"\"\n",
    "        compound_names.append(compound_name)\n",
    "\n",
    "data = [{\"ID\": id_value, \"compound name\": comp_name} for id_value, comp_name in zip(ids, compound_names)]\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = [\"ID\", \"compound name\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Extracted data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VB-JK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"datasets/processed/urban/AG Battisti Verena P/VB-JK - Kirchebner Julia/VB-JK_raw.txt\"\n",
    "output_file = \"datasets/processed/urban/AG Battisti Verena P/VB-JK - Kirchebner Julia/VB-JK_extracted.csv\"\n",
    "\n",
    "ids = []\n",
    "compound_names = []\n",
    "\n",
    "pattern_id = r\"^VB-JK\\d{3}\"\n",
    "\n",
    "pattern_compound = r\"^(.*)\\s+\\(X{0,2}\\d{1,2}\\)$\"\n",
    "\n",
    "with open(input_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if re.search(pattern_id, line):\n",
    "        ids.append(line.strip())\n",
    "        \n",
    "        if i >= 2:\n",
    "            match = re.match(pattern_compound, lines[i - 2].strip())\n",
    "            if match:\n",
    "                compound_name = match.group(1)\n",
    "            else:\n",
    "                compound_name = \"\"\n",
    "        else:\n",
    "            compound_name = \"\"\n",
    "        compound_names.append(compound_name)\n",
    "\n",
    "data = [{\"ID\": id_value, \"compound name\": comp_name} for id_value, comp_name in zip(ids, compound_names)]\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = [\"ID\", \"compound name\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Extracted data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_file, output_file, pdf_path_template):\n",
    "    ids = []\n",
    "    compound_names = []\n",
    "    solvent = \"d6DMSO\"\n",
    "    nmr_type = \"1H, 13C\"\n",
    "\n",
    "    with open(input_file, \"r\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            ids.append(row[\"ID\"])\n",
    "            compound_names.append(row[\"compound name\"])\n",
    "\n",
    "    updated_data = []\n",
    "    for i in range(len(ids)):\n",
    "        updated_data.append({\n",
    "            \"ID\": ids[i],\n",
    "            \"compound name\": compound_names[i],\n",
    "            \"Solvent\": solvent,\n",
    "            \"NMR-Type\": nmr_type,\n",
    "            \"PDF file path\": pdf_path_template.replace(\"XXX\", ids[i].split('-')[1])\n",
    "        })\n",
    "\n",
    "    with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"ID\", \"compound name\", \"Solvent\", \"NMR-Type\", \"PDF file path\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(updated_data)\n",
    "\n",
    "    print(f\"Additional columns added to {output_file}\")\n",
    "\n",
    "files_info = [\n",
    "    (\"datasets/processed/urban/AG Battisti Verena P/Gekaufte Substanzen/Gekaufte Substanzen_raw.csv\", \n",
    "     \"datasets/processed/urban/AG Battisti Verena P/Gekaufte Substanzen/Gekaufte Substanzen_extracted.csv\", \n",
    "     \"datasets/raw/urban/Sicherung Ausw 2023-08-24/AG Battisti Verena/Gekaufte Substanzen/VB-XXX.pdf\"), \n",
    "     (\"datasets/processed/urban/AG Battisti Verena P/VB- Battisti Verena/VB_raw.csv\", \n",
    "      \"datasets/processed/urban/AG Battisti Verena P/VB- Battisti Verena/VB_extracted.csv\", \n",
    "      \"datasets/raw/urban/Sicherung Ausw 2023-08-24/AG Battisti Verena/VB- Battisti Verena/VB-XXX.pdf\")\n",
    "      (\"datasets/processed/urban/AG Battisti Verena P/VB-JK - Kirchebner Julia/VB-JK_extracted.csv\", \n",
    "       \"datasets/processed/urban/AG Battisti Verena P/VB-JK - Kirchebner Julia/VB-JK_processed.csv\",\n",
    "       \"datasets/raw/urban/Sicherung Ausw 2023-08-24/AG Battisti Verena/VB-JK - Kirchebner Julia/VB-XXX.pdf\")\n",
    "]\n",
    "\n",
    "for input_file, output_file, pdf_template in files_info:\n",
    "    process_csv(input_file, output_file, pdf_template)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AG Lubec Gert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MK - Kirchhofer Michael, PN - Neill Philip, SB - Bittner Stefan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"datasets/processed/urban/AG Lubec Gert P/MK - Kirchhofer Michael/MK_raw.txt\"\n",
    "output_file = \"datasets/processed/urban/AG Lubec Gert P/MK - Kirchhofer Michael/MK_extracted.csv\"\n",
    "\n",
    "ids = []\n",
    "compound_names = []\n",
    "\n",
    "pattern_id = r\"^MK\\d{3}[\\w\\d-]*\"\n",
    "\n",
    "pattern_compound = r\"^(.*)$\"\n",
    "\n",
    "with open(input_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if re.search(pattern_id, line):\n",
    "        ids.append(line.strip())\n",
    "\n",
    "        if i >= 2 and i - 2 < len(lines):\n",
    "            compound_name = re.match(pattern_compound, lines[i - 2].strip())\n",
    "            if compound_name:\n",
    "                compound_names.append(compound_name.group(1))\n",
    "            else:\n",
    "                compound_names.append(\"\")\n",
    "\n",
    "data = [{\"ID\": id_value, \"compound name\": comp_name} for id_value, comp_name in zip(ids, compound_names)]\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = [\"ID\", \"compound name\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Extracted data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"datasets/processed/urban/AG Lubec Gert P/PN - Neill Philip/PN_raw.txt\"\n",
    "output_file = \"datasets/processed/urban/AG Lubec Gert P/PN - Neill Philip/PN_extracted.csv\"\n",
    "\n",
    "ids = []\n",
    "compound_names = []\n",
    "\n",
    "pattern_id = r\"^PN\\d{3}[\\w\\d-]*\"\n",
    "\n",
    "pattern_compound = r\"^(.*)$\"\n",
    "\n",
    "with open(input_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if re.search(pattern_id, line):\n",
    "        ids.append(line.strip())\n",
    "\n",
    "        if i >= 2 and i - 2 < len(lines):\n",
    "            compound_name = re.match(pattern_compound, lines[i - 2].strip())\n",
    "            if compound_name:\n",
    "                compound_names.append(compound_name.group(1))\n",
    "            else:\n",
    "                compound_names.append(\"\")\n",
    "\n",
    "data = [{\"ID\": id_value, \"compound name\": comp_name} for id_value, comp_name in zip(ids, compound_names)]\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = [\"ID\", \"compound name\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Extracted data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"datasets/processed/urban/AG Lubec Gert P/SB - Bittner Stefan/SB_raw.txt\"\n",
    "output_file = \"datasets/processed/urban/AG Lubec Gert P/SB - Bittner Stefan/SB_extracted.csv\"\n",
    "\n",
    "ids = []\n",
    "compound_names = []\n",
    "\n",
    "pattern_id = r\"^SB\\d{3}[\\w\\d-]*\"\n",
    "\n",
    "pattern_compound = r\"^(.*)$\"\n",
    "\n",
    "with open(input_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if re.search(pattern_id, line):\n",
    "        ids.append(line.strip())\n",
    "\n",
    "        if i >= 2 and i - 2 < len(lines):\n",
    "            compound_name = re.match(pattern_compound, lines[i - 2].strip())\n",
    "            if compound_name:\n",
    "                compound_names.append(compound_name.group(1))\n",
    "            else:\n",
    "                compound_names.append(\"\")\n",
    "\n",
    "data = [{\"ID\": id_value, \"compound name\": comp_name} for id_value, comp_name in zip(ids, compound_names)]\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = [\"ID\", \"compound name\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Extracted data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_file, output_file, pdf_path_template):\n",
    "    ids = []\n",
    "    compound_names = []\n",
    "    solvent = \"cdcl3\"\n",
    "    nmr_type = \"1H, 13C\"\n",
    "\n",
    "    # Read the input CSV file\n",
    "    with open(input_file, \"r\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            ids.append(row[\"ID\"])\n",
    "            compound_names.append(row[\"compound name\"])\n",
    "\n",
    "    # Prepare updated data with additional columns\n",
    "    updated_data = []\n",
    "    for i in range(len(ids)):\n",
    "        updated_data.append({\n",
    "            \"ID\": ids[i],\n",
    "            \"compound name\": compound_names[i],\n",
    "            \"Solvent\": solvent,\n",
    "            \"NMR-Type\": nmr_type,\n",
    "            \"PDF file path\": pdf_path_template.format(ID=ids[i])\n",
    "        })\n",
    "\n",
    "    # Write the updated data to the output CSV file\n",
    "    with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"ID\", \"compound name\", \"Solvent\", \"NMR-Type\", \"PDF file path\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(updated_data)\n",
    "\n",
    "    print(f\"Additional columns added to {output_file}\")\n",
    "\n",
    "# Define file paths and templates\n",
    "files_info = [\n",
    "    (\"datasets/processed/urban/AG Lubec Gert P/MK - Kirchhofer Michael/MK_extracted.csv\", \n",
    "     \"datasets/processed/urban/AG Lubec Gert P/MK - Kirchhofer Michael/MK_processed.csv\", \n",
    "     \"datasets/raw/urban/Sicherung Ausw 2023-08-24/AG Lubec Gert/MK - Kirchhofer Michael/{ID}.pdf\")\n",
    "    (\"datasets/processed/urban/AG Lubec Gert P/PN - Neill Philip/PN_extracted.csv\",\n",
    "     \"datasets/processed/urban/AG Lubec Gert P/PN - Neill Philip/PN_processed.csv\",\n",
    "     \"datasets/raw/urban/Sicherung Ausw 2023-08-24/AG Lubec Gert/PN - Neill Philip/{ID}.pdf\"),\n",
    "    (\"datasets/processed/urban/AG Lubec Gert P/SB - Bittner Stefan/SB_extracted.csv\",\n",
    "     \"datasets/processed/urban/AG Lubec Gert P/SB - Bittner Stefan/SB_processed.csv\",\n",
    "     \"datasets/raw/urban/Sicherung Ausw 2023-08-24/AG Lubec Gert/SB - Bittner Stefan/{ID}.pdf\")\n",
    "]\n",
    "\n",
    "# Process each pair of files\n",
    "for input_file, output_file, pdf_template in files_info:\n",
    "    process_csv(input_file, output_file, pdf_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating SMILES strings from IUPAC names in CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for generating the SMILES strings from IUPAC names\n",
    "\n",
    "CIR_BASE_URL = \"https://cactus.nci.nih.gov/chemical/structure\"\n",
    "\n",
    "# List of CSV files containing IUPAC names in column \"compound name\"\n",
    "INPUT_CSV_FILES = [\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/Gekaufte Substanzen/Gekaufte Substanzen_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/VB- Battisti Verena/VB_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/VB-JK - Kirchebner Julia/VB-JK_processed.csv\", \n",
    "    \"datasets/processed/urban/AG Lubec Gert P/MK - Kirchhofer Michael/MK_processed.csv\", \n",
    "    \"datasets/processed/urban/AG Lubec Gert P/PN - Neill Philip/PN_processed.csv\", \n",
    "    \"datasets/processed/urban/AG Lubec Gert P/SB - Bittner Stefan/SB_processed.csv\",\n",
    "]\n",
    "\n",
    "for input_csv_file in INPUT_CSV_FILES:\n",
    "    df = pd.read_csv(input_csv_file)\n",
    "\n",
    "    smiles_list = []\n",
    "\n",
    "    for iupac_name in df[\"compound name\"]:\n",
    "        try:\n",
    "            response = requests.get(f\"{CIR_BASE_URL}/{iupac_name}/smiles\", timeout=300)  \n",
    "            if response.status_code == 200:\n",
    "                smiles = response.text\n",
    "                smiles_list.append(smiles)\n",
    "            else:\n",
    "                smiles_list.append(\"Unable to retrieve SMILES\")\n",
    "                print(f\"Failed to retrieve SMILES for {iupac_name}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # In case CACTUS is down \n",
    "            smiles_list.append(\"Unable to retrieve SMILES\")\n",
    "            print(f\"Server down or connection unsuccessful for compound: {iupac_name}, Error: {e}\")\n",
    "\n",
    "    df.insert(df.columns.get_loc(\"compound name\") + 1, \"SMILES\", smiles_list)\n",
    "\n",
    "    df.to_csv(input_csv_file, index=False)\n",
    "\n",
    "    print(f\"SMILES added to {input_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning: Urban files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\"datasets/processed/urban/AG Battisti Verena P/Gekaufte Substanzen/Gekaufte Substanzen_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/VB- Battisti Verena/VB_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/VB-JK - Kirchebner Julia/VB-JK_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Lubec Gert P/MK - Kirchhofer Michael/MK_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Lubec Gert P/PN - Neill Philip/PN_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Lubec Gert P/SB - Bittner Stefan/SB_processed.csv\"]  \n",
    "\n",
    "for csv_file in csv_files:\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    nonexistent_pdf_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        pdf_path = row['PDF file path']\n",
    "\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"CSV File: {csv_file}\")\n",
    "            print(f\"PDF file path does not exist: {pdf_path}\")\n",
    "            print(f\"Row {index + 1}: {row}\")\n",
    "            nonexistent_pdf_count += 1\n",
    "\n",
    "    print(f\"Total rows with nonexistent PDFs in {csv_file}: {nonexistent_pdf_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\"datasets/processed/urban/AG Battisti Verena P/Gekaufte Substanzen/Gekaufte Substanzen_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/VB- Battisti Verena/VB_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/VB-JK - Kirchebner Julia/VB-JK_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Lubec Gert P/PN - Neill Philip/PN_processed.csv\"]  \n",
    "\n",
    "for csv_file in csv_files:\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    nonexistent_pdf_count = 0\n",
    "\n",
    "    cleaned_rows = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        pdf_path = row['PDF file path']\n",
    "\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"CSV File: {csv_file}\")\n",
    "            print(f\"PDF file path does not exist: {pdf_path}\")\n",
    "            print(f\"Row {index + 1}: {row}\")\n",
    "            nonexistent_pdf_count += 1\n",
    "        else:\n",
    "            cleaned_rows.append(row)\n",
    "\n",
    "    print(f\"Total rows with nonexistent PDFs in {csv_file}: {nonexistent_pdf_count}\")\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_rows, columns=df.columns)\n",
    "\n",
    "    cleaned_csv_file = csv_file.replace('.csv', '_cleaned.csv')\n",
    "    cleaned_df.to_csv(cleaned_csv_file, index=False)\n",
    "    print(f\"Cleaned CSV saved as {cleaned_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [ \n",
    "    \"datasets/processed/orts/iNEXT_Lib_1D/iNEXTG2-Plate-01.csv\",\n",
    "    \"datasets/processed/orts/iNEXT_Lib_1D/iNEXTG2-Plate-02.csv\",\n",
    "    \"datasets/processed/orts/iNEXT_Lib_1D/iNEXTG2-Plate-03.csv\",\n",
    "    \"datasets/processed/orts/iNEXT_Lib_1D/iNEXTG2-Plate-04.csv\",\n",
    "    \"datasets/processed/orts/iNEXT_Lib_1D/iNEXTG2-Plate-05.csv\",\n",
    "    \"datasets/processed/orts/iNEXT_Lib_1D/iNEXTG2-Plate-06.csv\",\n",
    "    \"datasets/processed/orts/iNEXT_Lib_1D/iNEXTG2-Plate-07.csv\",\n",
    "    \"datasets/processed/orts/iNEXT_Lib_1D/iNEXTG2-Plate-08.csv\",\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/Gekaufte Substanzen/Gekaufte Substanzen_processed_cleaned.csv\",\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/VB- Battisti Verena/VB_processed_cleaned.csv\",\n",
    "    \"datasets/processed/urban/AG Battisti Verena P/VB-JK - Kirchebner Julia/VB-JK_processed_cleaned.csv\",\n",
    "    \"datasets/processed/urban/AG Lubec Gert P/MK - Kirchhofer Michael/MK_processed.csv\",\n",
    "    \"datasets/processed/urban/AG Lubec Gert P/PN - Neill Philip/PN_processed_cleaned.csv\",\n",
    "    \"datasets/processed/urban/AG Lubec Gert P/SB - Bittner Stefan/SB_processed.csv\",\n",
    "    \"datasets/processed/nmrshiftdb2/nmredata/nmrshiftdb2.nmredata.csv\",\n",
    "    \"datasets/processed/nmrshiftdb2/withsignals/nmrshiftdb2withsignals.csv\"\n",
    "]\n",
    "\n",
    "output_folder = \"datasets/combined/\"\n",
    "\n",
    "last_assigned_id = {}\n",
    "\n",
    "dfs = []\n",
    "\n",
    "def generate_unique_id(row_index, folder_prefix):\n",
    "    if folder_prefix not in last_assigned_id:\n",
    "        last_assigned_id[folder_prefix] = 0\n",
    "    last_assigned_id[folder_prefix] += 1\n",
    "    return folder_prefix + str(last_assigned_id[folder_prefix]).zfill(5)\n",
    "\n",
    "for file_path in input_files:\n",
    "    if file_path.endswith(\".csv\"):\n",
    "        folder, filename = os.path.split(file_path)\n",
    "        folder_parts = folder.split(os.path.sep)\n",
    "        \n",
    "        for part in folder_parts:\n",
    "            if part in folder_prefixes:\n",
    "                folder_prefix = folder_prefixes[part]\n",
    "                break\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        df['Unique ID'] = [generate_unique_id(i, folder_prefix) for i in range(1, len(df) + 1)]\n",
    "        \n",
    "        dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "combined_df = combined_df[['Unique ID'] + [col for col in combined_df.columns if col != 'Unique ID']]\n",
    "\n",
    "combined_csv_path = os.path.join(output_folder, \"combined_all.csv\")\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    \"Unique ID\", \"ID\", \"Solvent\", \"SMILES\", \"NMR-Type\", \"SDF file path\",\n",
    "    \"PDF file path\", \"Image folder location\"\n",
    "]\n",
    "\n",
    "extracted_df = combined_df[columns_to_keep]\n",
    "\n",
    "extracted_csv_path = os.path.join(output_folder, \"combined_extracted.csv\")\n",
    "extracted_df.to_csv(extracted_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
